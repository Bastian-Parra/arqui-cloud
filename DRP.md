# üõ°Ô∏è Plan de Recuperaci√≥n ante Desastres (DRP)

## 1. Objetivo

Definir las estrategias, responsables y mecanismos necesarios para restaurar los servicios cr√≠ticos del sistema frente a desastres que afecten su disponibilidad, integridad o continuidad, minimizando p√©rdida de datos (RPO) e interrupciones (RTO).

## 2. Alcance

El DRP cubre los siguientes componentes:

- Microservicios
- Base de Datos (`Locales con MySQL`)
- API Gateway y balanceadores
- Almacenamiento estatico y backups
- Servicios de CI/CD en GitHub Actions
- GCP: Cloud Run, Cloud Storage y Pub/Sub.

## 3. Evaluaci√≥n de riesgos

| Categor√≠a         | Ejemplos                                            | Estrategia de mitigaci√≥n                                                                  |
| ----------------- | --------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| **F√≠sicos**       | Terremoto, incendio en regi√≥n de GCP                | Uso de m√∫ltiples zonas de disponibilidad y backups en regiones distintas                  |
| **Tecnol√≥gicos**  | Ca√≠da de Cloud SQL, bugs cr√≠ticos en microservicios | CI/CD con rollback, pruebas automatizadas, supervisi√≥n de errores                         |
| **Humanos**       | Borrado accidental, deploy incorrecto               | Pol√≠ticas de revisi√≥n (PRs obligatorias), backups, versionamiento y auditor√≠as            |
| **Seguridad/Red** | DDoS, filtraci√≥n de credenciales                    | WAF, IAM granular, escaneo con Gitleaks, vigilancia activa con Cloud Monitoring y alertas |

## 4. RPO/RTO

| Componente                      | Justificaci√≥n del RPO                      | RPO          | Justificaci√≥n del RTO                            | RTO        |
| ------------------------------- | ------------------------------------------ | ------------ | ------------------------------------------------ | ---------- |
| `Cloud SQL`                     | Respaldo diario + binlog activo            | 15 minutos   | Export disponible + replica lista en standby     | 20 minutos |
| `Firestore / Storage`           | Export diario a bucket multi-regi√≥n        | 1 hora       | Rehidrataci√≥n r√°pida desde snapshot/export       | 15 minutos |
| `Microservicios (Node.js)`      | No hay p√©rdida si el c√≥digo est√° en GitHub | 5 minutos    | Imagenes Docker + deploy automatizado            | 10 minutos |
| `Secret Manager / .env`         | Versionado manual + backups                | 5 minutos    | Recuperable desde GCP o repositorio cifrado      | 10 minutos |
| `API Gateway` / Balanceadores   | Configuraci√≥n declarativa en Terraform     | 0            | Redeploy inmediato con Terraform                 | 5 minutos  |
| **Sistema completo (failover)** | Considerando interacci√≥n entre componentes | \~15 minutos | Activaci√≥n manual + Terraform + restauraci√≥n SQL | 30 minutos |

## 5. Estrategia de recuperacion ante desastres

### Warm Standby (Servidores listos, pero no activos)

- Se mantienen bases de datos replicadas y microservicios construidos, pero el entorno de recuperaci√≥n no est√° corriendo constantemente (se activa al fallar el entorno principal).
- Permite reducir costos frente a una soluci√≥n Hot Standby (activa en paralelo), manteniendo tiempos de recuperaci√≥n razonables.

### Flujo de recuperaci√≥n por escenario

üî¥ Escenario 1: P√©rdida de base de datos (ej. corrupci√≥n o borrado accidental)
1. Se activa alerta v√≠a Cloud Monitoring.
2. Se restaura √∫ltimo snapshot autom√°tico de Cloud SQL (o export `.sql.gz`).
3. Terraform reconfigura los secrets/configuraci√≥n si es necesario.
4. Se ejecutan tests para validar conexi√≥n DB ‚Üí microservicios ‚Üí API.

üî¥ Escenario 2: Ca√≠da total de regi√≥n GCP (ej. terremoto o interrupci√≥n severa)
1. Se activa protocolo de failover manual.
2. Se recrea la infraestructura en regi√≥n secundaria (`us-central1` ‚Üí `southamerica-east1` por ejemplo), reutilizando:
    - Buckets con backups exportados
    - Docker images en Artifact Registry
    - Repositorio de infraestructura (Terraform)
3. Se redirige el tr√°fico mediante configuraci√≥n del API Gateway global.
4. Se revalida funcionamiento completo del sistema y se comunica recuperaci√≥n a stakeholders.

üî¥ Escenario 3: Vulneraci√≥n de seguridad (ej. exposici√≥n de secretos, malware)
1. Se revocan tokens y claves mediante Secret Manager.
2. Se activa escaneo con Gitleaks y auditor√≠a con Cloud Logging.
3. Se realiza rollback a √∫ltimo estado sano (commit firmado, imagen segura).
4. Se reconstruye todo el entorno de producci√≥n y se regeneran secretos.

### Herramientas utilizadas

| Componente               | Herramienta                                  | Uso                                      |
| ------------------------ | -------------------------------------------- | ---------------------------------------- |
| Backup DB                | Cloud SQL automatic backups + exports        | RPO controlado                           |
| Infraestructura          | Terraform + GitHub Actions                   | Reprovisi√≥n automatizada                 |
| Registro de eventos      | Cloud Logging + Monitoring                   | Detecci√≥n temprana                       |
| Seguridad                | Secret Manager + Gitleaks CI                 | Prevenci√≥n y respuesta ante filtraciones |

